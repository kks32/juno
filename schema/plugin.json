{
  "jupyter.lab.shortcuts": [
    {
      "command": "@juno/plugin:run_gpt",
      "keys": ["Accel Space"],
      "selector": ".jp-Cell"
    }
  ],
  "title": "juno",
  "description": "juno settings.",
  "type": "object",
  "jupyter.lab.menus": {
    "main": [
      {
        "id": "jp-mainmenu-edit",
        "items": [
          {
            "command": "@juno/plugin:run_gpt",
            "rank": 500
          }
        ]
      }
    ]
  },
  "properties": {
    "openai_key": {
      "type": "string",
      "title": "Open AI API Key",
      "description": "Your Open AI key to run GPT models.",
      "default": ""
    },
    "text_model": {
      "type": "string",
      "title": "Text Model",
      "description": "Default large language model to use",
      "default": "text-davinci-003"
    },
    "max_tokens": {
      "type": "integer",
      "title": "Max Tokens",
      "description": "The maximum number of tokens to generate in the completion.  The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). (From OpenAI Documentation)",
      "default": 256
    },
    "temperature": {
      "type": "number",
      "title": "Temperature",
      "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Alter this or `top_p`, not both. (From OpenAI Documentation)",
      "default": 0.7
    },
    "top_p": {
      "type": "number",
      "title": "top_p",
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Alter this or `temperature`, not both. (From OpenAI Documentation)",
      "default": 1.0
    },
    "presence_penalty": {
      "type": "number",
      "title": "Presence Penalty",
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.(From OpenAI Documentation)",
      "default": 0
    },
    "frequency_penalty": {
      "type": "number",
      "title": "Frequency Penalty",
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.(From OpenAI Documentation)",
      "default": 0
    }
  },
  "additionalProperties": false
}
